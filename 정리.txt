1. ollama에서 llama2 다운 및 설치
    - 사용방법 : ollama run llama2
2. llama2 사용방법
    - streamlit 사용 : streamlit run 파일명

ollama 설치 방법
    - curl -fsSL https://ollama.com/install.sh | sh

--------------------------

1. llama2 학습해보기
    1. huggingface에서 모델 다운 
        - llama-2-7B 사용하려고 했으나 config 파일을 찾이 못함
            - 검색해본 결과 다른 사람들도 config 파일이 없어서 다른 버전 사용
        - llama-2-7B-hf 사용

    2. huggingface의 학습 데이터 사용
        - Datasets이라고 명시되어진것 사용해야함

    3. nvidia와 amd를 돌리는 방법이 다름
        - amd 보다는 nvidia가 더 좋은 성능과 쉬운 방법을 가진걸로 파악
    
    4. CPU로 학습 
        4.1. 모델을 Local에 다운받아서 실행하려고 했더니 CPU 성능 문제로 에러 발생
            - huggingface의 URL 주소로 변경해서 실행했더니 성공
            +- 성능 문제인지 로컬에서는 죽음 유지보수 서버에서 진행할 예정

        4.2 서버에서 진행
            - 진행하며 수정한 사항
                - peft_config에 SFTConfig가 잘못 전달, SFTTrainer가 전달되도록 수정
                - SFTConfig의 초기화 후 output_dir 인자 필요
                - SFTTrainer에 config 객체를 전달하지 않고, 필요한 설정을 개별적으로 전달되게 수정
                - 데이터셋 필드값을 받아와서 넣는것으로 수정
                - SFTConfig 관련 부분을 제거하면서 SFTTrainer의 설정들을 직접 전달하는 방식으로 수정
                - CPU대신 CPU사용으로 인한 training_arguments 부분 수정

------------------------------
fine-tuning 2.1 변경된 것
 - 변경한 이유 : 2 버전에서 학습하려던 데이터가 너무 커서 CPU에서는 시간이 너무 오래걸림
                데이터를 변경하면서 데이터 형식이 변해서 그에 맞춤
                숫자형 데이터 -> 문자형으로 변환
 
- 변경한 것 -
dataset_text_field 자동 설정: 데이터셋의 텍스트 필드명을 자동으로 추출하도록 개선
옵티마이저 기본값 변경: optim을 adamw_hf로 설정하여 기본 옵티마이저를 사용하게 변경
모델 학습 중 캐시 비활성화: model.config.use_cache = False를 통해 학습 중 캐시 사용을 비활성화
메모리 최적화: low_cpu_mem_usage=True로 CPU에서 메모리 사용을 감소
merge_and_unload() 호출: LoRA 가중치를 병합하여 모델을 최종적으로 훈련된 상태로 저장하고 로드하는 방식으로 개선
packing 비활성화

- LoRA 가중치를 병합하는 과정에서 오류 발생
    - 활성 어댑터 수동 설정하는 걸로 진행

------------------------------

promt 사용
1. langchain 없이 promtp 사용 가능
    - 예) app_20240912_prompt사용버전.py

2. langchain 사용해 promtp 사용
    - 예) app_20240912_prompt_langchain사용버전.py